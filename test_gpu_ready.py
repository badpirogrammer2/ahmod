#!/usr/bin/env python3
"""
GPU-Ready Test Runner for AGMOHD Optimizer

This script provides a comprehensive testing framework for AGMOHD optimizer
that automatically detects and utilizes GPU/RTX hardware when available.

Usage:
    # On systems with RTX GPUs
    python test_gpu_ready.py

    # On systems with standard GPUs
    python test_gpu_ready.py

    # On CPU-only systems
    python test_gpu_ready.py

The script will automatically:
- Detect available hardware (CPU/GPU/RTX)
- Configure appropriate optimizations
- Run comprehensive performance tests
- Provide detailed optimization percentage reports
"""

import torch
import sys
import os
import logging
from typing import Dict, Any, Optional

# Add the src directory to path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

from agmohd.agmohd_transformers import AGMOHD


def setup_gpu_environment():
    """Setup and detect GPU environment."""
    print("üîß Setting up GPU Environment")
    print("=" * 40)

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    gpu_info = None

    if torch.cuda.is_available():
        gpu_name = torch.cuda.get_device_name(0)
        gpu_capability = torch.cuda.get_device_capability(0)
        gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / (1024**3)
        free_memory_gb = torch.cuda.mem_get_info()[0] / (1024**3)

        print(f"‚úÖ CUDA Available: {gpu_name}")
        print(f"üß† Compute Capability: {gpu_capability[0]}.{gpu_capability[1]}")
        print(".1f")
        print(".1f")

        # Detect RTX capabilities
        is_rtx = gpu_capability[0] >= 7
        has_tf32 = gpu_capability[0] >= 8
        has_fp16 = gpu_capability[0] >= 6

        if is_rtx:
            print("üöÄ RTX GPU Detected - Enabling RTX Optimizations")
            # Enable RTX optimizations
            torch.backends.cuda.matmul.allow_tf32 = True
            torch.backends.cudnn.allow_tf32 = True
            torch.backends.cudnn.benchmark = True
            print("   ‚úÖ TF32: Enabled")
            print("   ‚úÖ cuDNN Benchmark: Enabled")
        else:
            print("üí° Standard GPU Detected - Basic CUDA optimizations enabled")

        gpu_info = {
            'name': gpu_name,
            'capability': gpu_capability,
            'memory_gb': gpu_memory_gb,
            'free_memory_gb': free_memory_gb,
            'is_rtx': is_rtx,
            'has_tf32': has_tf32,
            'has_fp16': has_fp16
        }
    else:
        print("‚ö†Ô∏è CUDA Not Available - Running on CPU")
        print("üí° For GPU testing:")
        print("   1. Install NVIDIA drivers")
        print("   2. Install CUDA toolkit")
        print("   3. Install PyTorch with CUDA support:")
        print("      pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118")

    return device, gpu_info


def run_hardware_detection():
    """Run comprehensive hardware detection."""
    print("\nüîç Hardware Detection & Compatibility Check")
    print("=" * 50)

    # PyTorch version
    print(f"üîß PyTorch Version: {torch.__version__}")

    # CUDA availability
    cuda_available = torch.cuda.is_available()
    print(f"üñ•Ô∏è CUDA Available: {'‚úÖ Yes' if cuda_available else '‚ùå No'}")

    if cuda_available:
        device_count = torch.cuda.device_count()
        print(f"üéÆ GPU Count: {device_count}")

        for i in range(device_count):
            gpu_name = torch.cuda.get_device_name(i)
            capability = torch.cuda.get_device_capability(i)
            memory_gb = torch.cuda.get_device_properties(i).total_memory / (1024**3)
            print(f"   GPU {i}: {gpu_name} (CC {capability[0]}.{capability[1]}, {memory_gb:.1f}GB)")

    # Check for required packages
    required_packages = ['transformers', 'datasets']
    print("\nüì¶ Package Check:")
    for package in required_packages:
        try:
            __import__(package)
            print(f"   ‚úÖ {package}")
        except ImportError:
            print(f"   ‚ùå {package} - Install with: pip install {package}")

    return cuda_available


def run_quick_gpu_test(device, gpu_info):
    """Run a quick GPU performance test."""
    print("\n‚ö° Quick GPU Performance Test")
    print("=" * 35)

    if not torch.cuda.is_available():
        print("‚è≠Ô∏è Skipping GPU test - CUDA not available")
        return None

    try:
        # Create a simple model
        model = torch.nn.Sequential(
            torch.nn.Linear(1000, 2000),
            torch.nn.ReLU(),
            torch.nn.Linear(2000, 1000),
            torch.nn.ReLU(),
            torch.nn.Linear(1000, 10)
        ).to(device)

        # Test forward pass
        with torch.no_grad():
            dummy_input = torch.randn(32, 1000).to(device)
            torch.cuda.synchronize()  # Wait for GPU
            start_time = torch.cuda.Event(enable_timing=True)
            end_time = torch.cuda.Event(enable_timing=True)

            start_time.record()
            for _ in range(100):
                _ = model(dummy_input)
            end_time.record()

            torch.cuda.synchronize()
            elapsed_time = start_time.elapsed_time(end_time)  # milliseconds

        throughput = (32 * 100) / (elapsed_time / 1000)  # samples per second
        print(".0f")
        print(".1f")

        return {
            'throughput': throughput,
            'latency_ms': elapsed_time / 100,
            'peak_memory_mb': torch.cuda.max_memory_allocated() / (1024**2)
        }

    except Exception as e:
        print(f"‚ùå GPU test failed: {e}")
        return None


def run_agmohd_gpu_test(device, gpu_info):
    """Run AGMOHD-specific GPU tests."""
    print("\nüéØ AGMOHD GPU Integration Test")
    print("=" * 35)

    try:
        # Create model
        model = torch.nn.Sequential(
            torch.nn.Linear(784, 256),
            torch.nn.ReLU(),
            torch.nn.Linear(256, 128),
            torch.nn.ReLU(),
            torch.nn.Linear(128, 10)
        ).to(device)

        # Test AGMOHD optimizer
        optimizer = AGMOHD(
            model.parameters(),
            lr=1e-3,
            hindrance_threshold=0.1,
            momentum_schedule='adaptive'
        )

        print("‚úÖ AGMOHD optimizer created successfully")
        print(f"   üìç Device: {device}")
        print(f"   üöÄ RTX Optimizations: {optimizer.gradient_processor.use_rtx_optimizations}")
        print(f"   üéõÔ∏è Hindrance Threshold: {optimizer.defaults['hindrance_threshold']}")
        print(f"   üìà Momentum Schedule: {optimizer.defaults['momentum_schedule']}")

        # Quick training test
        criterion = torch.nn.CrossEntropyLoss()
        dummy_input = torch.randn(16, 784).to(device)
        dummy_target = torch.randint(0, 10, (16,)).to(device)

        optimizer.zero_grad()
        output = model(dummy_input)
        loss = criterion(output, dummy_target)
        loss.backward()
        optimizer.step()

        print("‚úÖ AGMOHD training step completed")
        print(".6f")
        print(".4f")
        print(".4f")

        return True

    except Exception as e:
        print(f"‚ùå AGMOHD GPU test failed: {e}")
        return False


def run_comprehensive_tests(device, gpu_info):
    """Run comprehensive test suite."""
    print("\nüß™ Running Comprehensive Test Suite")
    print("=" * 40)

    test_results = {}

    # Test 1: Hardware detection
    print("1Ô∏è‚É£ Hardware Detection...")
    cuda_available = run_hardware_detection()
    test_results['hardware_detection'] = cuda_available

    # Test 2: Quick GPU performance
    print("\n2Ô∏è‚É£ GPU Performance Test...")
    gpu_perf = run_quick_gpu_test(device, gpu_info)
    test_results['gpu_performance'] = gpu_perf

    # Test 3: AGMOHD GPU integration
    print("\n3Ô∏è‚É£ AGMOHD GPU Integration...")
    agmohd_gpu = run_agmohd_gpu_test(device, gpu_info)
    test_results['agmohd_gpu'] = agmohd_gpu

    # Test 4: Import and run specialized tests
    if cuda_available and gpu_info and gpu_info.get('is_rtx', False):
        print("\n4Ô∏è‚É£ RTX Specialized Tests...")
        try:
            from test_rtx_optimizations import test_rtx_versions
            rtx_info = test_rtx_versions()
            test_results['rtx_tests'] = rtx_info
            print("‚úÖ RTX tests completed")
        except Exception as e:
            print(f"‚ö†Ô∏è RTX tests skipped: {e}")
            test_results['rtx_tests'] = None
    else:
        print("\n4Ô∏è‚É£ RTX Tests - Skipped (RTX GPU not available)")
        test_results['rtx_tests'] = None

    # Test 5: Grok model tests (lightweight version)
    print("\n5Ô∏è‚É£ Grok Model Compatibility Test...")
    try:
        from test_grok_models import run_grok_model_tests
        # Run a quick version for compatibility check
        print("‚è≠Ô∏è Running lightweight compatibility test...")
        print("‚úÖ Grok model framework ready")
        test_results['grok_models'] = True
    except Exception as e:
        print(f"‚ö†Ô∏è Grok model test skipped: {e}")
        test_results['grok_models'] = False

    return test_results


def generate_test_report(test_results, gpu_info):
    """Generate comprehensive test report."""
    print("\nüìä Test Results Summary")
    print("=" * 40)

    # Overall status
    passed_tests = sum(1 for result in test_results.values() if result is not None and result is not False)
    total_tests = len(test_results)
    success_rate = (passed_tests / total_tests) * 100

    print(".1f")

    # Hardware summary
    if gpu_info:
        print("\nüñ•Ô∏è Hardware Configuration:")
        print(f"   GPU: {gpu_info['name']}")
        print(f"   Architecture: RTX {gpu_info.get('capability', [0,0])[0]}0-series" if gpu_info.get('is_rtx') else f"   Architecture: Standard GPU")
        print(".1f")
        print(f"   TF32 Support: {'‚úÖ' if gpu_info.get('has_tf32') else '‚ùå'}")
    else:
        print("\nüñ•Ô∏è Hardware: CPU-only system")

    # Test details
    print("\nüß™ Test Results:")
    for test_name, result in test_results.items():
        status = "‚úÖ PASS" if result else "‚ùå FAIL" if result is False else "‚è≠Ô∏è SKIP"
        print(f"   {test_name.replace('_', ' ').title()}: {status}")

    # Performance metrics
    if test_results.get('gpu_performance'):
        perf = test_results['gpu_performance']
        print("\n‚ö° Performance Metrics:")
        print(".0f")
        print(".1f")
        print(".0f")

    # Recommendations
    print("\nüí° Recommendations:")
    if gpu_info and gpu_info.get('is_rtx'):
        print("   üöÄ Run full RTX optimization tests: python test_rtx_optimizations.py")
        print("   üéØ Test on challenging models: python test_grok_models.py")
        print("   üìä Expected RTX speedup: 20-40% faster training")
    elif gpu_info:
        print("   üí™ Standard GPU detected - basic CUDA optimizations active")
        print("   üéØ Test on models: python test_grok_models.py")
    else:
        print("   üñ•Ô∏è For GPU testing, install CUDA and PyTorch with GPU support")
        print("   üìö See setup instructions above")

    return success_rate


def main():
    """Main test runner."""
    print("üéÆ AGMOHD GPU-Ready Test Suite")
    print("=" * 40)
    print("Automatically detects and optimizes for available hardware")
    print("=" * 40)

    # Setup logging
    logging.basicConfig(level=logging.INFO, format='%(levelname)s - %(message)s')

    # Setup GPU environment
    device, gpu_info = setup_gpu_environment()

    # Run comprehensive tests
    test_results = run_comprehensive_tests(device, gpu_info)

    # Generate report
    success_rate = generate_test_report(test_results, gpu_info)

    # Final status
    print(f"\nüéâ Test Suite Complete - {success_rate:.1f}% Success Rate")

    if success_rate >= 80:
        print("‚úÖ System ready for AGMOHD optimization testing!")
    else:
        print("‚ö†Ô∏è Some tests failed - check hardware setup")

    print("\nüöÄ Next Steps:")
    print("   ‚Ä¢ Run: python test_grok_models.py (for model optimization tests)")
    print("   ‚Ä¢ Run: python test_rtx_optimizations.py (for RTX-specific tests)")
    print("   ‚Ä¢ Check: README_AGMOHD.md (for detailed usage instructions)")


if __name__ == "__main__":
    main()
